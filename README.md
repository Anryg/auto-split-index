# auto-split-index
通过读取kafka的数据后，用spark streaming写入Elasticsearch，当数据量到达一定数量之后自动分索引

该模块使用的技术组件以及版本如下：

1，consul：1.5.2  ##用于模块中需要读取的所有配置信息

2，kafka：2.0.0   ##数据源存储

3，spark：2.3.2   ##计算引擎

4，Elasticsearch：6.8.1  ##数据最终存储

5，yarn：3.1.1   ##spark on yarn的方式进行分布式计算

6，HDFS：3.1.1   ##分布式文件系统

该模块解决的是：用spark streaming分布式计算引擎读取kafka的数据进过计算处理后向Elasticsearch的索引进行数据写入后，当索引的数据量到达一定量（比如过亿之后），该索引的查询性能一定会受到影响，无论是对该索引的一般查询还是进行聚合查询，效率会非常低下。因此，该模块会根据索引的数据量来判断是否需要分索引，如果数据量达到阈值后，系统会新建一个索引，则新的数据会入到新索引中，且老索引会生成一个带该索引时间范围的别名，并将该别名写入到另外一个索引中，作为二级索引，方便每次根据时间范围的查找方式，后续业务数据如果根据时间条件进行查询时候，会先查询具有二级索引功能的这个index，找到符合条件的目标索引，然后再从目标索引中再根据其他条件查询到目标数据。

