# auto-split-index
通过读取kafka的数据后，用spark streaming写入Elasticsearch，当数据量到达一定数量之后自动分索引

该模块解决的是，用spark streaming分布式计算引擎向Elasticsearch的索引进行数据写入后，当索引的数据量到达一定量（比如过亿之后），该索引的查询性能一定会受到影响，无论是对该索引的一般查询还是进行聚合查询，效率会非常低下。因此，该模块会根据索引的数据量来判断是否需要分索引，如果数据量达到阈值后，系统会新建一个索引，则新的数据会入到新索引中，且老索引会生成一个带该索引时间范围的别名，并将该别名写入到另外一个索引中，作为二级索引，方便每次根据时间范围的查找方式。

